Данный проект посвящен исследованию и оптимизации генеративно-состязательных сетей (GAN) для задачи генерации изображений рукописных цифр. 
Я создал 2 версии реализации этой модели.
  1. Базовая модель (Simple_GAN) Анализ обучения:D Loss: Снизился до 1.2068. Дискриминатор быстро научился отличать реальные изображения от подделок.G Loss: Упал с 2.18 до 1.0372.Наблюдения: Несмотря на падение лосса, качество генерации оставалось нестабильным. Без нормализации внутренних слоев градиенты становятся шумными, что приводит к появлению артефактов на изображениях.
  2. Улучшенная модель (Improved_GAN) Во второй версии кода я  добавил Batch Normalization. Это нормализует входные данные для каждого слоя. Результаты обучения:D Loss: Зафиксировался на уровне 1.2131.G Loss: Сбалансировался в районе 1.2866.Динамика: В отличие от первой модели, тут когда лосс дискриминатора немного растет, генератор получает шанс выучить более тонкие детали.

Вывод: BatchNorm: Использование пакетной нормализации позволило генератору быстрее находить правильную структуру цифр. Изображения стали менее «рваными».Баланс сил: Сглаживание меток помогло избежать ситуации, когда дискриминатор становится «слишком умным». Это подтверждается тем, что финальный лосс генератора во второй модели чуть выше, но качество картинки — лучше. В GAN низкий лосс не всегда означает хорошее качество. Итог: Для работы с изображениями даже в простых полносвязных сетях механизмы нормализации и регуляризации меток являются обязательными для получения качественного результата.
